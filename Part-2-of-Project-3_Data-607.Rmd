---
title: "Part 1 of Project 3"
subtitle: "Team - Data Domination"
author: "Nfn Tenzin Dakar, Nwe Oo Mon, Crystal Quezada"
date: "2024-10-07"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Table of Contents

## Members {.tabset}

- Nfn Tenzin Dakar
- Nwe Oo Mon (Nina)
- Crystal Quezada

## Description {.tabset}

Our group, DATA DOMINATION, will use Zoom and text messages for communication. For code sharing and documentation, we’re using RStudio Cloud with R Markdown published on RPubs. All files, including data and documentation, are stored in a GitHub repo. We've created an entity-relationship diagram (ERD) using Mysql DataBase and identified our data sources, which will be loaded via CSV files from GitHub.

## Collaboration Tools {.tabset}

Our team leverages several tools to streamline collaboration and project development:

•	RStudio Cloud ([rstudio.cloud](https://rstudio.cloud)): This platform facilitates collaborative code development, allowing all team members to view, edit, and share code seamlessly in real-time.

•	R Markdown in RStudio Cloud: We use R Markdown to document our project, and publish the results through RPubs ([rpubs.com](https://rpubs.com)), making our work easily accessible.

•	MySQL : This tool was employed to design the entity-relationship diagram (ERD) for the project.

•	[GitHub Repository](https://github.com/Nweoomon/Project3_607) :
All source CSV files and R Markdown (RMD) files are stored centrally on GitHub, allowing the team to access and manage files efficiently.

•	Communication: Our discussions and updates happen over Zoom for meetings and text messages for daily communication, ensuring smooth coordination.

## Sources {.tabset}
Kaggle ML & DS Survey: 
Our dataset is job listings based on software, which can be found on [Kaggle](https://www.kaggle.com/code/kerneler/starter-ds-job-listing-technology-04cdb0b7-0/input), it is also stored in our collaborative [GitHub repository](https://github.com/Nweoomon/Project3_607/blob/main/ds_job_listing_software.csv). 

The dataset presents job market data related to various programming and data tools, comparing their demand across different job platforms: LinkedIn, Indeed, SimplyHired, Monster, etc. The table shows the number of job postings mentioning each keyword (e.g., Python, R, SQL) on different platforms and provides the percentage of the total postings each keyword represents on those platforms.

## Entity Relationship Diagram {.tabset}

•	Our diagram can be found on our shared [GitHub Repository](https://github.com/Nweoomon/Project3_607/blob/main/Job_listing_ERD.jpg).

![Job_listing_ERD](https://github.com/Nweoomon/Project3_607/raw/main/Job_listing_ERD.jpg)
In our ER diagram, we created three tables: Keyword, JobSource, and Metrics.

The Keyword table stores the list of programming languages or technologies (e.g., Python, SQL, R), while the JobSource table holds different job platforms (e.g., LinkedIn, Indeed, Monster). The Metrics table acts as an associative entity that breaks down the many-to-many relationship between Keyword and Job Source into two one-to-many relationships.

One-to-many between Keyword and Metrics: 

For instance, "Python" can appear in multiple metrics entries, each representing a different job source (e.g., LinkedIn, Indeed). 

One-to-many between Job Source and Metrics: 

For example, LinkedIn can have metrics for various keywords (e.g., Python, SQL, R). 

Metrics as the Associative Entity 

Metrics has a foreign key to Keyword (keyword_id) and a foreign key to JobSource (source_id). 

• Our sql codes can be found on our shared [GitHub Repository](https://github.com/Nweoomon/Project3_607/blob/main/Job_listing.sql).

## MySQL Data Import {.tabset}

Data from MySQL will be imported to R in following steps:

Install and Load Necessary Packages: First, R packages DBI and RMariaDB are installed and loaded to connect and interact with the MySQL database.

#### Installing and Loading Required Packages
```{r installpackage}
if (!requireNamespace("DBI", quietly = TRUE)) install.packages("DBI")
if (!requireNamespace("RMariaDB", quietly = TRUE)) install.packages("RMariaDB")
if (!requireNamespace("tidyverse", quietly = TRUE)) install.packages("tidyverse")

library(DBI)
library(RMariaDB)
library(tidyverse)
```

Store the Database Password Securely: To avoid exposing the database password in the code, the Password was stored in an environment file (.Renviron). 

Connect to the MySQL Database: Using dbConnect(), a connection will established to the MySQL database by providing the necessary credentials (user, password, database name, host, and port).

Load Data from the Database: Once connected, SQL queries will be used to fetch data from specific tables. The result is stored in an R dataframe.

Close the Connection: After loading the data, the database will be disconnected using dbDisconnect() to free up resources.

```{r connect-to-mysql, echo=TRUE, message=FALSE, warning=FALSE}

con <- dbConnect(
  RMariaDB::MariaDB(),
  user = 'root',               
  password = Sys.getenv("MYSQL_PWD"),    
  dbname = 'job_listing',     
  host = 'localhost',           
  port = 3306                   
)

# Check if the connection is successful
if (!is.null(con)) {
  print("Connected successfully!")
} else {
  print("Connection failed.")
}

# Load the data from the MySQL table into an R dataframe
jobsource <- dbGetQuery(con, "SELECT * FROM jobsource")
keyword <- dbGetQuery(con, "SELECT * FROM keyword")
metrics <- dbGetQuery(con, "SELECT * FROM metrics")

# Execute the SQL query and fetch the results into an R data frame
query <- "SELECT keyword.keyword_name, 
                jobsource.source_name, 
                metrics.count, 
                metrics.percentage
        FROM metrics
        JOIN jobsource ON metrics.source_id = jobsource.source_id
        JOIN keyword ON metrics.keyword_id = keyword.keyword_id;"

# Fetch the data
metrics_combined <- dbGetQuery(con, query)

# Display the results
glimpse (metrics_combined)

# Close the database connection
dbDisconnect(con)
```

## Data Clean Up and Transformation {.tabset}
## Data Analysis {.tabset}

#### Total jobs by source

```{r most jobs}
job_counts <- metrics_combined %>%
  group_by(source_name) %>%
  summarize(Total_Count = sum(count, na.rm = TRUE), .groups = 'drop')

ggplot(job_counts, aes(x = reorder(source_name, Total_Count), y = Total_Count, fill = source_name)) +
  geom_bar(stat = "identity") +
  labs(x = "Job Source", y = "Total Job Count", title = "Total Job Counts by Job Source") +
  theme_minimal() +
  geom_text(aes(label = Total_Count), vjust = -0.5, size = 3)
```

LinkedIn has the greatest amount of jobs. Followed by Indeed.

#### Top skills

```{r top skills}
top_skills <- metrics_combined %>%
  arrange(desc(count)) %>%
  slice_head(n = 10)

ggplot(top_skills, aes(x = source_name, y = count, fill = keyword_name)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = count), vjust = -1, size = 2.65, position = position_dodge(0.9), size = 3) +
  theme_minimal() +
  labs(title = "Job Listings by Keyword and Job Source",
       x = "Job Source",
       y = "Number of Job Listings") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

The top skills are Python, R, and SQL, which can be seen on LinkedIn the most.

#### Bottom skills

```{r bottom skills}
bottom_keywords <- metrics_combined %>%
  group_by(keyword_name, source_name) %>%
  summarize(Total_Count = sum(count), .groups = 'drop') %>% 
  arrange(Total_Count) %>% # Arrange in ascending order to get lowest counts
  slice_head(n = 10)

ggplot(bottom_keywords, aes(x = reorder(keyword_name, Total_Count), y = Total_Count, fill = source_name)) +
  geom_bar(stat = "identity", position = "dodge") + # Position bars side by side
  labs(x = "Keyword", y = "Count", title = "Bottom 10 Keywords by Job Source Count") +
  theme_minimal() +
  geom_text(aes(label = Total_Count), vjust = -0.5, position = position_dodge(0.9), size = 3)
```

Of the "worst" skills, or skills with the least amount of job offerings are D3, Caffe, PyTorch, MongoDB, MySQL, Keras, and Cassandra. Between the "worst" performing job sites, SimplyHired does not have any job listings with the skills.

#### Correlation Analysis between platforms

```{r corr_sources}

metrics_clean <- metrics_combined %>%
  select(-percentage)

# Pivot the data to a wide format based on 'source_name'
wide_data <- metrics_clean %>%
  pivot_wider(names_from = source_name, values_from = count)

# View the reshaped data
print(wide_data)

# Compute the Pearson correlation matrix
cor_matrix <- cor(wide_data [,-1], use = "complete.obs", method = "pearson")

# Print the correlation matrix
print(cor_matrix)
```

## Conclusion {.tabset}


